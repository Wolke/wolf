/**
 * OpenAI å®¢æˆ¶ç«¯å°è£
 * @module services/ai/openai
 */

import OpenAI from 'openai';

/** OpenAI é…ç½® */
export interface OpenAIConfig {
    apiKey: string;
    model?: string;
}

/** é è¨­æ¨¡å‹ */
export const DEFAULT_MODEL = 'gpt-4o-mini';

/** OpenAI å®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆå–®ä¾‹ï¼‰*/
let openaiClient: OpenAI | null = null;
let currentApiKey: string = '';

/**
 * åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
 */
export function initializeOpenAI(config: OpenAIConfig): OpenAI {
    if (openaiClient && currentApiKey === config.apiKey) {
        return openaiClient;
    }

    openaiClient = new OpenAI({
        apiKey: config.apiKey,
        dangerouslyAllowBrowser: true, // å…è¨±å‰ç«¯ä½¿ç”¨
    });
    currentApiKey = config.apiKey;

    return openaiClient;
}

/**
 * å–å¾— OpenAI å®¢æˆ¶ç«¯
 */
export function getOpenAIClient(): OpenAI | null {
    return openaiClient;
}

/**
 * æª¢æŸ¥ OpenAI æ˜¯å¦å·²åˆå§‹åŒ–
 */
export function isOpenAIInitialized(): boolean {
    return openaiClient !== null;
}

/**
 * å‘¼å« Chat Completion API
 */
export async function chatCompletion(
    messages: OpenAI.Chat.ChatCompletionMessageParam[],
    options?: {
        model?: string;
        temperature?: number;
        maxTokens?: number;
    }
): Promise<string> {
    if (!openaiClient) {
        throw new Error('OpenAI å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« initializeOpenAI');
    }

    console.log('\nğŸ“¤ ======== OpenAI API Request ========');
    console.log('ğŸ“Œ Model:', options?.model || DEFAULT_MODEL);
    console.log('ğŸ“ Messages:');
    messages.forEach((msg, i) => {
        console.log(`  [${i}] ${msg.role}:`, msg.content);
    });
    console.log('========================================\n');

    const response = await openaiClient.chat.completions.create({
        model: options?.model || DEFAULT_MODEL,
        messages,
        temperature: options?.temperature ?? 0.7,
        max_tokens: options?.maxTokens ?? 500,
    });

    const content = response.choices[0]?.message?.content;

    console.log('\nğŸ“¥ ======== OpenAI API Response ========');
    console.log('ğŸ“¦ Content:', content);
    console.log('ğŸ“Š Usage:', response.usage);
    console.log('=========================================\n');

    if (!content) {
        throw new Error('OpenAI è¿”å›ç©ºå…§å®¹');
    }

    return content;
}

/**
 * å‘¼å« Chat Completion APIï¼ˆJSON æ¨¡å¼ï¼‰
 */
export async function chatCompletionJSON<T>(
    messages: OpenAI.Chat.ChatCompletionMessageParam[],
    options?: {
        model?: string;
        temperature?: number;
        maxTokens?: number;
    }
): Promise<T> {
    if (!openaiClient) {
        throw new Error('OpenAI å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« initializeOpenAI');
    }

    console.log('\nğŸ“¤ ======== OpenAI API Request (JSON) ========');
    console.log('ğŸ“Œ Model:', options?.model || DEFAULT_MODEL);
    console.log('ğŸ“ Messages:');
    messages.forEach((msg, i) => {
        console.log(`  [${i}] ${msg.role}:`, typeof msg.content === 'string' ? msg.content.substring(0, 200) + '...' : msg.content);
    });
    console.log('================================================\n');

    const response = await openaiClient.chat.completions.create({
        model: options?.model || DEFAULT_MODEL,
        messages,
        temperature: options?.temperature ?? 0.7,
        max_tokens: options?.maxTokens ?? 500,
        response_format: { type: 'json_object' },
    });

    const content = response.choices[0]?.message?.content;

    console.log('\nğŸ“¥ ======== OpenAI API Response (JSON) ========');
    console.log('ğŸ“¦ Content:', content);
    console.log('ğŸ“Š Usage:', response.usage);
    console.log('=================================================\n');

    if (!content) {
        throw new Error('OpenAI è¿”å›ç©ºå…§å®¹');
    }

    try {
        return JSON.parse(content) as T;
    } catch (error) {
        throw new Error(`OpenAI è¿”å›çš„ JSON è§£æå¤±æ•—: ${content}`);
    }
}

/**
 * ç°¡æ˜“ Prompt å‘¼å«
 */
export async function prompt(
    systemPrompt: string,
    userPrompt: string,
    options?: {
        model?: string;
        temperature?: number;
        maxTokens?: number;
    }
): Promise<string> {
    return chatCompletion(
        [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt },
        ],
        options
    );
}

/**
 * ç°¡æ˜“ Prompt å‘¼å«ï¼ˆJSON æ¨¡å¼ï¼‰
 */
export async function promptJSON<T>(
    systemPrompt: string,
    userPrompt: string,
    options?: {
        model?: string;
        temperature?: number;
        maxTokens?: number;
    }
): Promise<T> {
    return chatCompletionJSON<T>(
        [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt },
        ],
        options
    );
}
